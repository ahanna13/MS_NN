{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1J9X9tVi6IVnDRCGFufHmmJhvbiNjRExL",
      "authorship_tag": "ABX9TyNsCocrT0cQEz+9R0bkMKGe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahanna13/MS_NN/blob/main/MS_NN_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is my first trial at a neural net for my dataset from RKI:\n",
        "\n",
        "### Version 4.1 (20230306) of the MALDI-ToF Mass Spectrometry Database for Identification and Classification of Highly Pathogenic Microorganisms from the Robert Koch-Institute (RKI). It can be found at https://zenodo.org/records/7990990"
      ],
      "metadata": {
        "id": "CFg9YeoHp1mn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt7O44Vrpd_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7536cb2-5fb6-4274-b7f0-577f3f3591a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/129.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "pip install keras-tuner\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense"
      ],
      "metadata": {
        "id": "CNoRfLO5PLq1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ------ DATA PROCESSING ------"
      ],
      "metadata": {
        "id": "zhb9yBImnW-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This was what it gave me for a 2D input"
      ],
      "metadata": {
        "id": "O8ukV45xnGxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "data = pd.read_csv('your_file.txt', sep=\" \", header=None, names=['intensity'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_df, val_df = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# Create image data generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    directory='./',  # or your specific directory\n",
        "    x_col='x',\n",
        "    y_col='y',\n",
        "    target_size=(64, 64),  # Resize images\n",
        "    batch_size=32,\n",
        "    class_mode='binary')  # or 'categorical' for multi-class\n",
        "\n",
        "val_generator = val_datagen.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    directory='./',\n",
        "    x_col='x',\n",
        "    y_col='y',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='binary')"
      ],
      "metadata": {
        "id": "4Y0o45LZPdvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is what it gave me for a 1D input"
      ],
      "metadata": {
        "id": "8IbgskBnnNhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example loading and preprocessing\n",
        "data = pd.read_csv('your_file.txt', sep=\" \", header=None)\n",
        "X = pad_sequences(data['x'].values)\n",
        "y = to_categorical(data['y'].values)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "iF8lVwTwmiDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ------ BUILD CNN MODEL ------"
      ],
      "metadata": {
        "id": "9-xZBnx0nego"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    # input shape should be 66,000, 1\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)),  # Adjust input shape based on your data; Ved said I may not need 32 since its spectral peaks\n",
        "    MaxPooling1D(2),\n",
        "    Conv1D(64, 3, activation='relu'),\n",
        "    MaxPooling1D(2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'), # Ved said to add another hidden layer\n",
        "    # y_train.shape[1] should be 72 for the number of genera I have\n",
        "    Dense(y_train.shape[1], activation='softmax')  # Assuming y is one-hot encoded\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "3cYZzDCvnNQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ------ TRAIN MODEL ------"
      ],
      "metadata": {
        "id": "NgSrElRWoOIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure input data is correctly shaped\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "id": "FSP7_IsooWRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ------ EVALUATE MODEL ------"
      ],
      "metadata": {
        "id": "mxnoTqrIoZrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "dV_cKUZroZLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I need to add https://wandb.ai/site"
      ],
      "metadata": {
        "id": "6oG7sJeYotl2"
      }
    }
  ]
}