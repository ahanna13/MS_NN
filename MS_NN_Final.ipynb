{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1J9X9tVi6IVnDRCGFufHmmJhvbiNjRExL",
      "authorship_tag": "ABX9TyPhCEfs89WG+q1L9aN4Ms8O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahanna13/MS_NN/blob/main/MS_NN_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is my first trial at a neural net for my dataset from RKI:\n",
        "\n",
        "### Version 4.1 (20230306) of the MALDI-ToF Mass Spectrometry Database for Identification and Classification of Highly Pathogenic Microorganisms from the Robert Koch-Institute (RKI). It can be found at https://zenodo.org/records/7990990"
      ],
      "metadata": {
        "id": "CFg9YeoHp1mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DONT RUN THIS CELL ASH\n",
        "# THIS CELL WAS ORIGINALLY USED TO READ IN THE DATASET VIA FOR LOOPS\n",
        "# DO NOT NEED TO RUN AGAIN SINCE I SAVED IT TO A HDF5 IN A LATER CELL\n",
        "\n",
        "# trying to load the data using os.path and numpy\n",
        "import os\n",
        "import numpy as np\n",
        "#from tensorflow.keras.models import Sequential\n",
        "#from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "\n",
        "#from sklearn.preprocessing import MultiLabelBinarizer\n",
        "#from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# initializing a vector to append to\n",
        "labeled_vector = []\n",
        "\n",
        "# initializing the right size to also append to\n",
        "# 66000 total x axis points for all interpolated files; 9823 total files\n",
        "data = np.zeros((66000,9823),dtype='float')\n",
        "\n",
        "# set the base path to directory that holds all the genre folders each with many txt files\n",
        "#basedir = '/content/drive/MyDrive/ColabNotebooks/RKI_V4_1_ExtractedTXTs/Interpolated_RKI_V4_1/'\n",
        "basedir = \"C:\\\\Users\\\\Ashley Hanna\\\\Interpolated_RKI_V4_1\\\\\"\n",
        "\n",
        "# capture all the folder names since each of those is the label for the txts inside the folder\n",
        "# aka since I am trying to predict the genus of the microbe analyzed, the data is organized into folders for each genus\n",
        "labels = os.listdir(basedir)\n",
        "\n",
        "# initializing a counter\n",
        "col_num = 0\n",
        "\n",
        "for (root,dirs,files) in os.walk(basedir):\n",
        "    for label in labels:\n",
        "        if os.path.basename(root) == label:\n",
        "            for file in files:\n",
        "                print(os.path.join(root,file))\n",
        "                #df = pd.read_csv(os.path.join(root,file),header=None,sep=\" \",dtype='float')\n",
        "                df = np.loadtxt(os.path.join(root,file))\n",
        "                print(os.path.basename(root))\n",
        "                data[:,col_num] = df[:,1]\n",
        "                col_num += 1\n",
        "                print(col_num)\n",
        "                labeled_vector.append(os.path.basename(root))\n",
        "\n",
        "print(data)\n",
        "print(labeled_vector)"
      ],
      "metadata": {
        "id": "KTN0vYe5inPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DONT RUN THIS CELL ASH\n",
        "# THIS CELL WAS USED TO GRAB ONLY TWO GENRE TO SIMPLIFY THE NERUAL NET\n",
        "# THIS DROPS THE CLASSES FROM 73 TOTAL TO 2\n",
        "# 73 WAS TOO MANY TO START\n",
        "\n",
        "# THIS CELL DOES NOT NEED TO BE RUN AGAIN, SAVED TO H5PY FILES\n",
        "\n",
        "# convert to df with labels as first row and data as the rest\n",
        "#pd.DataFrame({data[:,i]:j for i,j in enumerate(labeled_vector)})\n",
        "\n",
        "#unique_genus_labels = np.unique(labeled_vector)\n",
        "\n",
        "# turning text labels into integer labels\n",
        "labeled_vector = np.array(labeled_vector)\n",
        "labeled_encoder = LabelEncoder()\n",
        "vec = labeled_encoder.fit_transform(labeled_vector)\n",
        "\n",
        "# grabbing the data for Brucella and Burkholderia spectra only\n",
        "# this for loop is just counting how many spectra there are in total\n",
        "# starting a counter\n",
        "j = 0\n",
        "new_data = []\n",
        "new_labeled_encoder = []\n",
        "for i in range(len(vec)):\n",
        "    if vec[i]==10: # reminder 6 is bacillus\n",
        "        #new_labeled_encoder[j] = 0 # calling bac \"0\"\n",
        "        #new_labeled_encoder.append(0)\n",
        "        #new_data[:,j]=data[:,i]\n",
        "        j = j + 1\n",
        "    if vec[i]==11:\n",
        "        #new_labeled_encoder[j] = 1 # calling bac \"0\"\n",
        "        #new_labeled_encoder.append(1)\n",
        "        #new_data[:,j]=data[:,i]\n",
        "        j = j + 1\n",
        "\n",
        "new_labeled_encoder = np.zeros(j)\n",
        "new_data = np.zeros((data.shape[0],j))\n",
        "j = 0\n",
        "\n",
        "# initializing counts for each genus\n",
        "Brucella = 0\n",
        "Burkholderia = 0\n",
        "\n",
        "# this for loop is actually grabbing the data out and storing it\n",
        "for i in range(len(vec)):\n",
        "    if vec[i]==10:\n",
        "        new_labeled_encoder[j] = 0\n",
        "        new_data[:,j]=data[:,i]\n",
        "        j = j + 1\n",
        "        Brucella = Brucella + 1\n",
        "    if vec[i] == 11:\n",
        "        new_labeled_encoder[j] = 1\n",
        "        new_data[:,j] = data[:,i]\n",
        "        j = j + 1\n",
        "        Burkholderia = Burkholderia + 1\n",
        "\n",
        "# print out how many there are of each genus\n",
        "print(Brucella)\n",
        "print(Burkholderia)"
      ],
      "metadata": {
        "id": "AuLz3LfAipI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DONT RUN THIS CELL ASH\n",
        "# THIS CELL IS USED TO INTEGER ENCODE AND THEN ONE HOT ENCODE MY LABELS\n",
        "# SHUFFLE OF DATA OCCURS HERE\n",
        "\n",
        "#from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#vec = labeled_encoder.fit_transform(labeled_vector)\n",
        "\n",
        "# shuffle data\n",
        "indx_rand = np.random.permutation(j)\n",
        "\n",
        "# integer encode\n",
        "vec = new_labeled_encoder[indx_rand]\n",
        "new_data = new_data[:,indx_rand]\n",
        "\n",
        "# checking the shape\n",
        "print(np.shape(vec))\n",
        "\n",
        "# one-hot-encoding\n",
        "one_hot_label = to_categorical(vec)\n",
        "\n",
        "#train_df, val_df = train_test_split(data, test_size=0.2)\n",
        "\n",
        "#X_train = np.transpose(data[:,0::2]); Y_train = one_hot_label[0::2,:];\n",
        "#X_test  = np.transpose(data[:,1::2]); Y_test  = one_hot_label[1::2,:];\n",
        "\n",
        "# x = data of the intensitites\n",
        "# data is a vector with every col being a spectrum\n",
        "# y = labels"
      ],
      "metadata": {
        "id": "vjbXbyINiwdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DONT RUN THIS CELL ASH\n",
        "# THIS CELL WAS WHAT WROTE THE DATASET TO A HDF5 FILE TYPE USING H5PY\n",
        "import h5py\n",
        "\n",
        "# 'w' means write\n",
        "# 'r' means read\n",
        "# https://docs.h5py.org/en/stable/high/file.html\n",
        "\n",
        "f = h5py.File('data_NN_class_project_3.hdf5','w')\n",
        "\n",
        "f[\"data\"] = new_data\n",
        "f[\"label\"] = one_hot_label\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "id": "HnGKECy9i1M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ START HERE!!!!!!!!!!!!!!!!!! ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# RUN THIS ONE ASH\n",
        "# THIS CELL IS WHAT LOADS IN THE DATASET FROM HDF5 FILES\n",
        "# RUN EVERY TIME KERNAL IS RELOADED\n",
        "import h5py\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "\n",
        "# to view and open data\n",
        "# https://myhdf5.hdfgroup.org/\n",
        "\n",
        "# Initialize arrays of the correct size for my data\n",
        "data_h5py = np.zeros([66000, 1833]) # intensities\n",
        "one_hot_label_h5py = np.zeros([1833, 2]) # labels\n",
        "\n",
        "# read in the data from files\n",
        "with h5py.File(\"C:\\\\Users\\\\Ashley Hanna\\\\data_NN_class_project_3.hdf5\", \"r\") as f:\n",
        "    data_h5py[:] = f[\"data\"] # save the intensities for later use\n",
        "    one_hot_label_h5py[:] = f[\"label\"] # save the labels for later use\n"
      ],
      "metadata": {
        "id": "RSv6pcCFi5F5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS ONE ASH\n",
        "# TRAIN-TEST-VALIDATION SPLIT OCCURS HERE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# typical to have the first paramater to be the number of the samples, second is number of features\n",
        "# create test train split\n",
        "#x_train, x_test, y_train, y_test = train_test_split(np.transpose(new_data), one_hot_label, test_size=0.33, random_state=42)\n",
        "x_train, x_test, y_train, y_test = train_test_split(np.transpose(data_h5py), one_hot_label_h5py, test_size=0.33, random_state=42)\n",
        "\n",
        "# create validation split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
        "\n",
        "#x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
        "#x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
        "#y_train = y_train.reshape((y_train.shape[0], y_train.shape[1], 1))\n",
        "#y_test = y_test.reshape((y_test.shape[0], y_test.shape[1], 1))\n",
        "#x_val = x_val.reshape((x_val.shape[0], x_val.shape[1], 1))\n",
        "#y_val = y_val.reshape((y_val.shape[0], y_val.shape[1], 1))\n"
      ],
      "metadata": {
        "id": "k9_bCHFyi8VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS ONE ASH\n",
        "# In Keras, model.fit requires two numpy arrays to train on - data, labels.\n",
        "# This works correctly if each row of the data has one corresponding label.\n",
        "\n",
        "model = Sequential([\n",
        "    # input shape should be 66,000, 1\n",
        "    Conv1D(4, 4, activation='relu', input_shape=(66000, 1)),  # 32 might be too big, try 8; may not need 32 since its spectral peaks\n",
        "    #MaxPooling1D(8),\n",
        "    #Conv1D(16, 16, activation='relu'),\n",
        "    #MaxPooling1D(8),\n",
        "    #Conv1D(8, 16, activation='relu'),\n",
        "    #MaxPooling1D(8),\n",
        "    Flatten(),\n",
        "    #Dense(2048, activation='relu'),\n",
        "    #Dense(256, activation='relu'),\n",
        "    #Dense(64, activation='relu'),# Ved said to add another hidden layer\n",
        "    # y_train.shape[1] should be 72 for the number of genera I have\n",
        "    Dense(2, activation='softmax')  # Assuming y is one-hot encoded\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "NJ6s1p0Si__k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DONT RUN THIS CELL ASH\n",
        "# Attempted Keras Tuner\n",
        "import keras_tuner as kt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # Tune the number of convolutional layers\n",
        "    for i in range(hp.Int('conv_layers', 1, 3, default=2)):\n",
        "        model.add(layers.Conv1D(\n",
        "            # number of filters aka number of patterns\n",
        "            filters=hp.Int(f'filters_{i}', 8, 64, step=8),\n",
        "            # kernal size aka filter size\n",
        "            kernel_size=hp.Int(f'kernel_size_{i}', 3, 10),\n",
        "            activation='relu',\n",
        "            # feeding in 1 spectra a time (aka 1 col)\n",
        "            # col length is 66000\n",
        "            # only looking at one input value for each m/z (channel)\n",
        "            input_shape=(66000, 1) if i == 0 else None,\n",
        "            padding='same'\n",
        "        ))\n",
        "        #print(layers.output_shape)\n",
        "       # model.add(layers.MaxPooling1D(pool_size=1))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Tune the number of dense layers\n",
        "    for i in range(hp.Int('dense_layers', 1, 3, default=2)):\n",
        "        model.add(layers.Dense(\n",
        "            units=hp.Int(f'units_{i}', min_value=32, max_value=512, step=32),\n",
        "            activation='relu'\n",
        "        ))\n",
        "\n",
        "    model.add(layers.Dense(73, activation='softmax'))\n",
        "\n",
        "    # Tune the learning rate\n",
        "    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "89fVsyitjD5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DONT RUN THIS CELL ASH\n",
        "# Create a tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,  # Number of different configurations to try\n",
        "    executions_per_trial=1,  # Number of models to train per configuration\n",
        "    directory='./model_tuning',  # Directory to store the models\n",
        "    project_name='keras_tuner_demo'\n",
        ")\n",
        "\n",
        "# Load your data here\n",
        "# x_train, y_train, x_val, y_val = ...\n",
        "\n",
        "\n",
        "\n",
        "# Start the tuning process\n",
        "tuner.search(\n",
        "    x_train, y_train,\n",
        "    epochs=10,\n",
        "    validation_data=(x_val, y_val)\n",
        ")\n",
        "\n",
        "# Get the best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Summary of the best model\n",
        "best_model.summary()"
      ],
      "metadata": {
        "id": "JeQW9phfjHgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS ONE ASH\n",
        "# TRAIN YOUR MODEL\n",
        "history = model.fit( #best_model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_data=(x_val, y_val),\n",
        "    #validation_split=0.1,\n",
        "    epochs=5,\n",
        "    batch_size=128\n",
        ")"
      ],
      "metadata": {
        "id": "xmCGJVWujL87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS ONE ASH\n",
        "# EVALUATE HOW IT DID VISUALLY W/ LOSS CURVE\n",
        "import matplotlib.pyplot as plt\n",
        "# import xarray as xr\n",
        "\n",
        "plt.plot(history.history[\"loss\"], c='k', label=\"training\")\n",
        "plt.plot(history.history[\"val_loss\"], c='r', label=\"validation\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.title('Loss Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xkBU1nLbjPy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS ONE ASH\n",
        "# EVALUATE HOW IT DID VISUALLY W/ ACCURACY CURVE\n",
        "plt.plot(history.history[\"accuracy\"], c='k', label=\"training\")\n",
        "plt.plot(history.history[\"val_accuracy\"], c='r', label=\"validation\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.title('Accuracy Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()\n",
        "\n",
        "# SHOULD BE PLOTTING CATEGORICAL CROSSENTROPY NOT ACCURACY\n",
        "# check in the RNNs slide deck"
      ],
      "metadata": {
        "id": "vDyhzJxTjSea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS ONE ASH\n",
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(x_test, y_test, batch_size=36)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "metadata": {
        "id": "dDqiOfoIjYCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix for NN\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# convert to integer labels\n",
        "#y_pred_int_labels = np.argmax(y_pred)#, axis=1)\n",
        "y_test_int_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_test_int_labels, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
        "\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix for NN')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GKLn1xRTjanW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC curve for NN\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the probabilities of each class\n",
        "y_probs = model.predict_proba(x_test)[:, 1]  # probabilities for the positive class\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test_int_labels, y_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic for NN')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WvsLC4bRjc2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance for NN\n",
        "importances = model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Plot the feature importances of the forest\n",
        "plt.figure()\n",
        "plt.title(\"Feature Importances for NN\")\n",
        "plt.bar(range(x_train.shape[1]), importances[indices],\n",
        "       color=\"r\", align=\"center\")\n",
        "plt.xticks(range(x_train.shape[1]), indices)\n",
        "plt.xlim([-1, x_train.shape[1]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HdLM2Jt2jfVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random forest for comparison\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_h5py_RF = data_h5py\n",
        "one_hot_label_h5py_RF = one_hot_label_h5py\n",
        "\n",
        "# transpose dataset to match up the lengths\n",
        "x_RF = np.transpose(data_h5py_RF)\n",
        "# Convert one-hot encoded labels to a single label vector\n",
        "y_RF = np.argmax(one_hot_label_h5py_RF, axis=1)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "x_train_RF, x_test_RF, y_train_RF, y_test_RF = train_test_split(x_RF, y_RF, test_size=0.25, random_state=42)\n",
        "\n",
        "# Create the random forest classifier\n",
        "RFmodel = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "RFmodel.fit(x_train_RF, y_train_RF)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = RFmodel.score(x_test_RF, y_test_RF)\n",
        "print(f'Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "id": "tMbXzQGCjiD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the random forest\n",
        "# Confusion matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "y_pred_RF = RFmodel.predict(x_test_RF)\n",
        "\n",
        "cm = confusion_matrix(y_test_RF, y_pred_RF)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=RFmodel.classes_)\n",
        "\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix for Random Forest')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DLyYJOKQjkxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the random forest\n",
        "# ROC curve\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the probabilities of each class\n",
        "y_probs_RF = RFmodel.predict_proba(x_test_RF)[:, 1]  # probabilities for the positive class\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test_RF, y_probs_RF)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic for Random Forest')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vS0EA8Htjnuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the random forest\n",
        "# Feature importance\n",
        "importances = RFmodel.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Plot the feature importances of the forest\n",
        "plt.figure()\n",
        "plt.title(\"Feature Importances for Random Forest\")\n",
        "plt.bar(range(x_train_RF.shape[1]), importances[indices],\n",
        "       color=\"r\", align=\"center\")\n",
        "plt.xticks(range(x_train_RF.shape[1]), indices)\n",
        "plt.xlim([-1, x_train_RF.shape[1]])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mHvcW5fWjq1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I need to add https://wandb.ai/site"
      ],
      "metadata": {
        "id": "6oG7sJeYotl2"
      }
    }
  ]
}